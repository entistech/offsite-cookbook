{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a680d8d-a9ea-4693-b886-858b32decd6c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Long Document Content Extraction\n",
    "\n",
    "GPT-3 can help us extract key figures, dates or other bits of important content from documents that are too big to fit into the context window. One approach for solving this is to chunk the document up and process each chunk separately, before combining into one list of answers. \n",
    "\n",
    "In this notebook we'll run through this approach:\n",
    "- Load in a long PDF and pull the text out\n",
    "- Create a prompt to be used to extract key bits of information\n",
    "- Chunk up our document and process each chunk to pull any answers out\n",
    "- Combine them at the end\n",
    "- This simple approach will then be extended to three more difficult questions\n",
    "\n",
    "## Approach\n",
    "\n",
    "- **Setup**: Take a PDF, a Formula 1 Financial Regulation document on Power Units, and extract the text from it for entity extraction. We'll use this to try to extract answers that are buried in the content.\n",
    "- **Simple Entity Extraction**: Extract key bits of information from chunks of a document by:\n",
    "    - Creating a template prompt with our questions and an example of the format it expects\n",
    "    - Create a function to take a chunk of text as input, combine with the prompt and get a response\n",
    "    - Run a script to chunk the text, extract answers and output them for parsing\n",
    "- **Complex Entity Extraction**: Ask some more difficult questions which require tougher reasoning to work out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1b82bd8-0803-4f9e-8e6f-8a8518629619",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18078c9d-289a-41b7-89c7-f59943f52d92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install textract\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b64dd6e4-94e6-4cad-a91c-f4cd54844da3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import textract\n",
    "import os\n",
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "# Extract the raw text from each PDF using textract\n",
    "text = textract.process('data/fia_f1_power_unit_financial_regulations_issue_1_-_2022-08-16.pdf', method='pdfminer').decode('utf-8')\n",
    "clean_text = text.replace(\"  \", \" \").replace(\"\\n\", \"; \").replace(';',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b65547f6-6cb5-4824-9c4d-9ef3381fc01a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Simple Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2b8368d-8513-4af3-ae45-4e96884e6cce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example prompt - \n",
    "document = '<document>'\n",
    "template_prompt=f'''Extract key pieces of information from this regulation document.\n",
    "If a particular piece of information is not present, output \\\"Not specified\\\".\n",
    "When you extract a key piece of information, include the closest page number.\n",
    "Use the following format:\\n0. Who is the author\\n1. What is the amount of the \"Power Unit Cost Cap\" in USD, GBP and EUR\\n2. What is the value of External Manufacturing Costs in USD\\n3. What is the Capital Expenditure Limit in USD\\n\\nDocument: \\\"\\\"\\\"{document}\\\"\\\"\\\"\\n\\n0. Who is the author: Tom Anderson (Page 1)\\n1.'''\n",
    "print(template_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab48ad9e-6481-4ab1-840d-29da335be735",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split a text into smaller chunks of size n, preferably ending at the end of a sentence\n",
    "def create_chunks(text, n, tokenizer):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    \"\"\"Yield successive n-sized chunks from text.\"\"\"\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens\n",
    "        j = min(i + int(1.5 * n), len(tokens))\n",
    "        while j > i + int(0.5 * n):\n",
    "            # Decode the tokens and check for full stop or newline\n",
    "            chunk = tokenizer.decode(tokens[i:j])\n",
    "            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n",
    "                break\n",
    "            j -= 1\n",
    "        # If no end of sentence found, use n tokens as the chunk size\n",
    "        if j == i + int(0.5 * n):\n",
    "            j = min(i + n, len(tokens))\n",
    "        yield tokens[i:j]\n",
    "        i = j\n",
    "\n",
    "def extract_chunk(document,template_prompt):\n",
    "    \n",
    "    prompt=template_prompt.replace('<document>',document)\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "    model='text-davinci-003', \n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=1500,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0\n",
    "    )\n",
    "    return \"1.\" + response['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "826c8db3-bf8a-4f93-8e93-350ebaec3588",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialise tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "results = []\n",
    "    \n",
    "chunks = create_chunks(clean_text,1000,tokenizer)\n",
    "text_chunks = [tokenizer.decode(chunk) for chunk in chunks]\n",
    "\n",
    "for chunk in text_chunks:\n",
    "    results.append(extract_chunk(chunk,template_prompt))\n",
    "    #print(chunk)\n",
    "    print(results[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73b35412-f917-41b0-ae7d-38285e101ecc",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "groups = [r.split('\\n') for r in results]\n",
    "\n",
    "# zip the groups together\n",
    "zipped = list(zip(*groups))\n",
    "zipped = [x for y in zipped for x in y if \"Not specified\" not in x and \"__\" not in x]\n",
    "zipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d733ac2-3e59-43a9-a329-48b9946e4465",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Complex Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d77ba4f2-a67c-4e21-8573-44a2eac8b693",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example prompt - \n",
    "template_prompt=f'''Extract key pieces of information from this regulation document.\n",
    "If a particular piece of information is not present, output \\\"Not specified\\\".\n",
    "When you extract a key piece of information, include the closest page number.\n",
    "Use the following format:\\n0. Who is the author\\n1. How is a Minor Overspend Breach calculated\\n2. How is a Major Overspend Breach calculated\\n3. Which years do these financial regulations apply to\\n\\nDocument: \\\"\\\"\\\"{document}\\\"\\\"\\\"\\n\\n0. Who is the author: Tom Anderson (Page 1)\\n1.'''\n",
    "print(template_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72f49473-50b5-4ee4-ae87-12820138831b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for chunk in text_chunks:\n",
    "    results.append(extract_chunk(chunk,template_prompt))\n",
    "    \n",
    "groups = [r.split('\\n') for r in results]\n",
    "\n",
    "# zip the groups together\n",
    "zipped = list(zip(*groups))\n",
    "zipped = [x for y in zipped for x in y if \"Not specified\" not in x and \"__\" not in x]\n",
    "zipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "011a50ae-36be-4a14-9e20-160e087def64",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Consolidation\n",
    "\n",
    "We've been able to extract the first two answers safely, while the third was confounded by the date that appeared on every page, though the correct answer is in there as well.\n",
    "\n",
    "To tune this further you can consider experimenting with:\n",
    "- A more descriptive or specific prompt\n",
    "- If you have sufficient training data, fine-tuning a model to find a set of outputs very well\n",
    "- The way you chunk your data - we have gone for 1000 tokens with no overlap, but more intelligent chunking that breaks info into sections, cuts by tokens or similar may get better results\n",
    "\n",
    "However, with minimal tuning we have now answered 6 questions of varying difficulty using the contents of a long document, and have a reusable approach that we can apply to any long document requiring entity extraction. Look forward to seeing what you can do with this!"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "Entity_extraction_for_long_documents",
   "notebookOrigID": 1572001650642153,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "embed_retrieve",
   "language": "python",
   "name": "embed_retrieve"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "5997d090960a54cd76552f75eca12ec3b416cf9d01a1a5af08ae48cf90878791"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
