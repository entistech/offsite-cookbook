{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be3c5419-d47d-4e3f-8115-b1499cea4e95",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Recommendation using embeddings and nearest neighbor search\n",
    "\n",
    "Recommendations are widespread across the web.\n",
    "\n",
    "- 'Bought that item? Try these similar items.'\n",
    "- 'Enjoy that book? Try these similar titles.'\n",
    "- 'Not the help page you were looking for? Try these similar pages.'\n",
    "\n",
    "This notebook demonstrates how to use embeddings to find similar items to recommend. In particular, we use [AG's corpus of news articles](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html) as our dataset.\n",
    "\n",
    "Our model will answer the question: given an article, what other articles are most similar to it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "235b8646-dbc1-4bd4-aab4-41f3f3dd9415",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1. Imports\n",
    "\n",
    "First, let's import the packages and functions we'll need for later. If you don't have these, you'll need to install them. You can install them via your terminal by running `pip install {package_name}`, e.g. `pip install pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9a48071-8ead-4340-84d3-d8584d130792",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from openai.embeddings_utils import (\n",
    "    get_embedding,\n",
    "    distances_from_embeddings,\n",
    "    tsne_components_from_embeddings,\n",
    "    chart_from_components,\n",
    "    indices_of_nearest_neighbors_from_distances,\n",
    ")\n",
    "\n",
    "# constants\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d44f356-68c5-4cab-b63c-fcfef46fcdf8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2. Load data\n",
    "\n",
    "Next, let's load the AG news data and see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5ac470a-5fc2-4a4e-994d-05951ea76ed1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load data (full dataset available at http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html)\n",
    "dataset_path = \"data/AG_news_samples.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# print dataframe\n",
    "n_examples = 5\n",
    "df.head(n_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "429ef09d-8792-4086-95ea-b2f3d47eca0d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's take a look at those same examples, but not truncated by ellipses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7702f9b3-84ea-49e2-b65c-c971a728e549",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# print the title, description, and label of each example\n",
    "for idx, row in df.head(n_examples).iterrows():\n",
    "    print(\"\")\n",
    "    print(f\"Title: {row['title']}\")\n",
    "    print(f\"Description: {row['description']}\")\n",
    "    print(f\"Label: {row['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12c51946-76d8-452f-8c3f-bcdf1ab69654",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3. Build cache to save embeddings\n",
    "\n",
    "Before getting embeddings for these articles, let's set up a cache to save the embeddings we generate. In general, it's a good idea to save your embeddings so you can re-use them later. If you don't save them, you'll pay again each time you compute them again.\n",
    "\n",
    "The cache is a dictionary that maps tuples of `(text, model)` to an embedding, which is a list of floats. The cache is saved as a Python pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cef5993-e1cd-41ba-a08a-0356cd156855",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# establish a cache of embeddings to avoid recomputing\n",
    "# cache is a dict of tuples (text, model) -> embedding, saved as a pickle file\n",
    "\n",
    "# set path to embedding cache\n",
    "embedding_cache_path = \"data/recommendations_embeddings_cache.pkl\"\n",
    "\n",
    "# load the cache if it exists, and save a copy to disk\n",
    "try:\n",
    "    embedding_cache = pd.read_pickle(embedding_cache_path)\n",
    "except FileNotFoundError:\n",
    "    embedding_cache = {}\n",
    "with open(embedding_cache_path, \"wb\") as embedding_cache_file:\n",
    "    pickle.dump(embedding_cache, embedding_cache_file)\n",
    "\n",
    "# define a function to retrieve embeddings from the cache if present, and otherwise request via the API\n",
    "def embedding_from_string(\n",
    "    string: str,\n",
    "    model: str = EMBEDDING_MODEL,\n",
    "    embedding_cache=embedding_cache\n",
    ") -> list:\n",
    "    \"\"\"Return embedding of given string, using a cache to avoid recomputing.\"\"\"\n",
    "    if (string, model) not in embedding_cache.keys():\n",
    "        embedding_cache[(string, model)] = get_embedding(string, model)\n",
    "        with open(embedding_cache_path, \"wb\") as embedding_cache_file:\n",
    "            pickle.dump(embedding_cache, embedding_cache_file)\n",
    "    return embedding_cache[(string, model)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fe5f48d-aebf-4b7f-a4b6-af993c1a7c40",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's check that it works by getting an embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68c4a828-94fd-41c8-9e3b-8f6ffe74b9b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# as an example, take the first description from the dataset\n",
    "example_string = df[\"description\"].values[0]\n",
    "print(f\"\\nExample string: {example_string}\")\n",
    "\n",
    "# print the first 10 dimensions of the embedding\n",
    "example_embedding = embedding_from_string(example_string)\n",
    "print(f\"\\nExample embedding: {example_embedding[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "497ca92b-c53c-4b5c-8ffe-c72d4965aaa8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 4. Recommend similar articles based on embeddings\n",
    "\n",
    "To find similar articles, let's follow a three-step plan:\n",
    "1. Get the similarity embeddings of all the article descriptions\n",
    "2. Calculate the distance between a source title and all other articles\n",
    "3. Print out the other articles closest to the source title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85500c81-169c-42ce-915e-b2c82f1be2ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def print_recommendations_from_strings(\n",
    "    strings: list[str],\n",
    "    index_of_source_string: int,\n",
    "    k_nearest_neighbors: int = 1,\n",
    "    model=EMBEDDING_MODEL,\n",
    ") -> list[int]:\n",
    "    \"\"\"Print out the k nearest neighbors of a given string.\"\"\"\n",
    "    # get embeddings for all strings\n",
    "    embeddings = [embedding_from_string(string, model=model) for string in strings]\n",
    "    # get the embedding of the source string\n",
    "    query_embedding = embeddings[index_of_source_string]\n",
    "    # get distances between the source embedding and other embeddings (function from embeddings_utils.py)\n",
    "    distances = distances_from_embeddings(query_embedding, embeddings, distance_metric=\"cosine\")\n",
    "    # get indices of nearest neighbors (function from embeddings_utils.py)\n",
    "    indices_of_nearest_neighbors = indices_of_nearest_neighbors_from_distances(distances)\n",
    "\n",
    "    # print out source string\n",
    "    query_string = strings[index_of_source_string]\n",
    "    print(f\"Source string: {query_string}\")\n",
    "    # print out its k nearest neighbors\n",
    "    k_counter = 0\n",
    "    for i in indices_of_nearest_neighbors:\n",
    "        # skip any strings that are identical matches to the starting string\n",
    "        if query_string == strings[i]:\n",
    "            continue\n",
    "        # stop after printing out k articles\n",
    "        if k_counter >= k_nearest_neighbors:\n",
    "            break\n",
    "        k_counter += 1\n",
    "\n",
    "        # print out the similar strings and their distances\n",
    "        print(\n",
    "            f\"\"\"\n",
    "        --- Recommendation #{k_counter} (nearest neighbor {k_counter} of {k_nearest_neighbors}) ---\n",
    "        String: {strings[i]}\n",
    "        Distance: {distances[i]:0.3f}\"\"\"\n",
    "        )\n",
    "\n",
    "    return indices_of_nearest_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39afb81b-136a-48cf-ae28-222e375d4dc9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5. Example recommendations\n",
    "\n",
    "Let's look for articles similar to first one, which was about Tony Blair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11e09d3a-e8e4-4f87-b321-d427a1bb33da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "article_descriptions = df[\"description\"].tolist()\n",
    "\n",
    "tony_blair_articles = print_recommendations_from_strings(\n",
    "    strings=article_descriptions,  # let's base similarity off of the article description\n",
    "    index_of_source_string=0,  # let's look at articles similar to the first one about Tony Blair\n",
    "    k_nearest_neighbors=5,  # let's look at the 5 most similar articles\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "042c9a2d-f8bd-46b2-b6db-edf28ba77493",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Pretty good! 4 of the 5 recommendations explicitly mention Tony Blair and the fifth is an article from London about climate change, topics that might be often associated with Tony Blair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ab506ad-1068-4870-9007-6b1230981a83",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's see how our recommender does on the second example article about NVIDIA's new chipset with more security."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f491adaa-8570-4e95-89c9-57685634521a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chipset_security_articles = print_recommendations_from_strings(\n",
    "    strings=article_descriptions,  # let's base similarity off of the article description\n",
    "    index_of_source_string=1,  # let's look at articles similar to the second one about a more secure chipset\n",
    "    k_nearest_neighbors=5,  # let's look at the 5 most similar articles\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b25cae0-cc1d-41ac-aa64-d10e87ed9a32",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "From the printed distances, you can see that the #1 recommendation is much closer than all the others (0.11 vs 0.14+). And the #1 recommendation looks very similar to the starting article - it's another article from PC World about increasing computer security. Pretty good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0f2a325-a904-4eae-86f9-01c260b2a5e3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Appendix: Using embeddings in more sophisticated recommenders\n",
    "\n",
    "A more sophisticated way to build a recommender system is to train a machine learning model that takes in tens or hundreds of signals, such as item popularity or user click data. Even in this system, embeddings can be a very useful signal into the recommender, especially for items that are being 'cold started' with no user data yet (e.g., a brand new product added to the catalog without any clicks yet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4ab41ba-9fe7-4c3b-8cc5-2e421f20d386",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Appendix: Using embeddings to visualize similar articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2505b0ba-2f12-49f5-a5ce-817735c1d7d5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To get a sense of what our nearest neighbor recommender is doing, let's visualize the article embeddings. Although we can't plot the 2048 dimensions of each embedding vector, we can use techniques like [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) or [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) to compress the embeddings down into 2 or 3 dimensions, which we can chart.\n",
    "\n",
    "Before visualizing the nearest neighbors, let's visualize all of the article descriptions using t-SNE. Note that t-SNE is not deterministic, meaning that results may vary from run to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d75ad56-379d-4380-912e-a9de7b3bd2cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get embeddings for all article descriptions\n",
    "embeddings = [embedding_from_string(string) for string in article_descriptions]\n",
    "# compress the 2048-dimensional embeddings into 2 dimensions using t-SNE\n",
    "tsne_components = tsne_components_from_embeddings(embeddings)\n",
    "# get the article labels for coloring the chart\n",
    "labels = df[\"label\"].tolist()\n",
    "\n",
    "chart_from_components(\n",
    "    components=tsne_components,\n",
    "    labels=labels,\n",
    "    strings=article_descriptions,\n",
    "    width=600,\n",
    "    height=500,\n",
    "    title=\"t-SNE components of article descriptions\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc56d004-75dc-491f-aaf0-efeb30e44781",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As you can see in the chart above, even the highly compressed embeddings do a good job of clustering article descriptions by category. And it's worth emphasizing: this clustering is done with no knowledge of the labels themselves!\n",
    "\n",
    "Also, if you look closely at the most egregious outliers, they are often due to mislabeling rather than poor embedding. For example, the majority of the blue World points in the green Sports cluster appear to be Sports stories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de46eb6d-9f91-4043-b1db-c85af7877c49",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Next, let's recolor the points by whether they are a source article, its nearest neighbors, or other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e117d42a-a0a4-4a5f-b774-e9d588c5dee7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create labels for the recommended articles\n",
    "def nearest_neighbor_labels(\n",
    "    list_of_indices: list[int],\n",
    "    k_nearest_neighbors: int = 5\n",
    ") -> list[str]:\n",
    "    \"\"\"Return a list of labels to color the k nearest neighbors.\"\"\"\n",
    "    labels = [\"Other\" for _ in list_of_indices]\n",
    "    source_index = list_of_indices[0]\n",
    "    labels[source_index] = \"Source\"\n",
    "    for i in range(k_nearest_neighbors):\n",
    "        nearest_neighbor_index = list_of_indices[i + 1]\n",
    "        labels[nearest_neighbor_index] = f\"Nearest neighbor (top {k_nearest_neighbors})\"\n",
    "    return labels\n",
    "\n",
    "\n",
    "tony_blair_labels = nearest_neighbor_labels(tony_blair_articles, k_nearest_neighbors=5)\n",
    "chipset_security_labels = nearest_neighbor_labels(chipset_security_articles, k_nearest_neighbors=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cff5d5a6-5355-4ecd-b66b-e828967d0acc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# a 2D chart of nearest neighbors of the Tony Blair article\n",
    "chart_from_components(\n",
    "    components=tsne_components,\n",
    "    labels=tony_blair_labels,\n",
    "    strings=article_descriptions,\n",
    "    width=600,\n",
    "    height=500,\n",
    "    title=\"Nearest neighbors of the Tony Blair article\",\n",
    "    category_orders={\"label\": [\"Other\", \"Nearest neighbor (top 5)\", \"Source\"]},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4f1b708-eff8-4ab9-af8b-edb4ffaf05dd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Looking at the 2D chart above, we can see that the articles about Tony Blair are somewhat close together inside of the World news cluster. Interestingly, although the 5 nearest neighbors (red) were closest in high dimensional space, they are not the closest points in this compressed 2D space. Compressing the embeddings down to 2 dimensions discards much of their information, and the nearest neighbors in the 2D space don't seem to be as relevant as those in the full embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a76157a-aedb-4927-9736-4317b646291f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# a 2D chart of nearest neighbors of the chipset security article\n",
    "chart_from_components(\n",
    "    components=tsne_components,\n",
    "    labels=chipset_security_labels,\n",
    "    strings=article_descriptions,\n",
    "    width=600,\n",
    "    height=500,\n",
    "    title=\"Nearest neighbors of the chipset security article\",\n",
    "    category_orders={\"label\": [\"Other\", \"Nearest neighbor (top 5)\", \"Source\"]},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83232e60-0710-4d66-a4d1-17b3f371a527",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For the chipset security example, the 4 closest nearest neighbors in the full embedding space remain nearest neighbors in this compressed 2D visualization. The fifth is displayed as more distant, despite being closer in the full embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20f13846-ce24-4452-adcd-3084ecec0a3f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Should you want to, you can also make an interactive 3D plot of the embeddings with the function `chart_from_components_3D`. (Doing so will require recomputing the t-SNE components with `n_components=3`.)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "Recommendation_using_embeddings",
   "notebookOrigID": 1572001650642658,
   "widgets": {}
  },
  "interpreter": {
   "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('openai': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
